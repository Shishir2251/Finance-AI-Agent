ğŸ“˜ RAG System with Gemini API â€” README.md
ğŸš€ Overview

This project is a Lightweight RAG (Retrieval-Augmented Generation) system that uses:

Gemini API (instead of OpenAI)

FAISS vector database

HuggingFace sentence-transformers for embeddings

Python CLI interface to ask questions

Chunk-based document retrieval

You can index any PDF/text â†’ chunk â†’ embed â†’ store in FAISS â†’ query using Gemini.

ğŸ§  How the System Works (End-to-End)
1ï¸âƒ£ Add PDFs or text

Place your PDFs inside:

data/raw_pdfs/

2ï¸âƒ£ Convert PDFs to text

A script extracts text and saves into:

data/processed_text/

3ï¸âƒ£ Chunk the text

Long documents are broken into small meaningful chunks (example: 300â€“500 tokens).

Output saved at:

data/chunks.json

4ï¸âƒ£ Build Vector DB

Each chunk is embedded using:

sentence-transformers/all-MiniLM-L6-v2


Then inserted into FAISS:

data/faiss.index

5ï¸âƒ£ Query the system

You ask:

python src/main.py --query "What is the cow referred to in Hinduism?"


System runs:

Embed your query

Retrieve the top relevant chunks

Send retrieved text + user query â†’ Gemini API

Gemini generates a grounded, accurate answer

ğŸ§© Folder Structure
project/
â”‚
â”œâ”€ src/
â”‚  â”œâ”€ main.py                   # CLI entrypoint
â”‚  â”œâ”€ rag.py                    # RAG pipeline using Gemini
â”‚  â”œâ”€ vectorstore.py            # FAISS loading/building
â”‚  â””â”€ pdf_loader.py             # PDF â†’ text processor
â”‚
â”œâ”€ data/
â”‚  â”œâ”€ raw_pdfs/                 # Add your PDFs here
â”‚  â”œâ”€ processed_text/           # Extracted text
â”‚  â”œâ”€ faiss.index               # Vector DB
â”‚  â””â”€ chunks.json               # Text chunks
â”‚
â”œâ”€ requirements.txt
â””â”€ README.md

ğŸ“¦ Installation
1. Create environment
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows

2. Install requirements
pip install -r requirements.txt

3. Add Gemini API Key

Set the environment variable:

export GEMINI_API_KEY="your_key_here"


Windows:

set GEMINI_API_KEY=your_key_here

ğŸ” How to Use the System
Step 1 â€” Add your content

Put PDFs inside:

data/raw_pdfs/

Step 2 â€” Preprocess PDFs

Example command:

python src/pdf_loader.py

Step 3 â€” Build Vector DB
python src/vectorstore.py


This generates:

faiss.index

chunks.json

Step 4 â€” Ask any question

Example:

python src/main.py --query "What is the cow referred to in Hinduism?" \
--index-path data/faiss.index \
--chunks-path data/chunks.json

Sample Output
Top relevant chunks foundâ€¦
Sending to Geminiâ€¦

Answer:
In Hinduism, the cow is referred to as "Gau Mata" (Mother Cow)â€¦

ğŸ”¥ Explanation of load_vector_db() (Your Latest Code)
db = FAISS.from_texts(docs, embedder)


What it does:

Loads all chunks from chunks.json

Creates embeddings using MiniLM

Stores vectors in FAISS index

Makes your RAG system ready for semantic search

ğŸ¤– Gemini Integration (RAG v2 Upgraded Pipeline)

Your upgraded RAG v2 uses:

Gemini 1.5 Flash / Pro

Fast response

Long context window

Best for RAG tasks

Flow:
query â†’ embed â†’ vector search â†’ retrieve â†’ Gemini â†’ final answer

ğŸ“ Test Questions For Your Document

Given your sample document about Holy Cow, here are testing questions:

Why is the cow considered sacred in Hinduism?

What does the term "Gau Mata" mean?

How is the cow connected with purity or fertility?

What role does the cow play in agrarian life?

What symbolism does the cow represent in ancient texts?

Why is venerating the cow seen as protecting nature?

ğŸ“š Example Query You Used
python src/main.py --query "What is the cow referred to in Hinduism?" \
--index-path data/faiss.index \
--chunks-path data/chunks.json
